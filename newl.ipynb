{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company Extraction Demo\n",
    "\n",
    "This notebook demonstrates the company extraction functionality with chunking and async processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import the extraction functions\n",
    "from extract_companies_optimized import (\n",
    "    process_article_async,\n",
    "    get_azure_client,\n",
    "    chunk_text,\n",
    "    process_chunk_async,\n",
    "    merge_chunk_results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load your company database\n",
    "company_database = pd.read_csv('company_database.csv')\n",
    "print(f\"Loaded {len(company_database)} companies from database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process a Single Article\n",
    "\n",
    "Let's process a single article to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example article text\n",
    "article_text = \"\"\"\n",
    "Apple Inc. (AAPL) reported strong earnings today. Microsoft (MSFT) and Google (GOOGL) also showed positive results. \n",
    "The tech sector was led by these companies, with Amazon (AMZN) and Meta (META) following closely behind.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "llm = get_azure_client()\n",
    "\n",
    "# Process the article\n",
    "results, metrics = await process_article_async(article_text, company_database, llm)\n",
    "\n",
    "print(\"\\nExtracted Companies:\")\n",
    "for company in results:\n",
    "    print(f\"- {company.get('CompanyName')} ({company.get('RIC')})\")\n",
    "\n",
    "print(\"\\nProcessing Metrics:\")\n",
    "print(json.dumps(metrics, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Multiple Articles\n",
    "\n",
    "Now let's process multiple articles in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load articles from a directory\n",
    "def load_articles(articles_dir: str):\n",
    "    articles = []\n",
    "    articles_path = Path(articles_dir)\n",
    "    \n",
    "    if not articles_path.exists():\n",
    "        raise FileNotFoundError(f\"Articles directory not found: {articles_dir}\")\n",
    "        \n",
    "    for file_path in articles_path.glob(\"*.txt\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                articles.append({\n",
    "                    \"id\": file_path.stem,\n",
    "                    \"content\": content,\n",
    "                    \"file_path\": str(file_path)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading article {file_path}: {e}\")\n",
    "            \n",
    "    return articles\n",
    "\n",
    "# Load articles\n",
    "articles = load_articles('articles')\n",
    "print(f\"Loaded {len(articles)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Process articles in parallel\n",
    "async def process_articles_batch(articles, batch_size=5):\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(articles), batch_size):\n",
    "        batch = articles[i:i + batch_size]\n",
    "        print(f\"\\nProcessing batch {i//batch_size + 1}/{(len(articles) + batch_size - 1)//batch_size}\")\n",
    "        \n",
    "        # Process batch concurrently\n",
    "        tasks = [\n",
    "            process_article_async(\n",
    "                article[\"content\"],\n",
    "                company_database,\n",
    "                llm,\n",
    "                use_cache=True\n",
    "            )\n",
    "            for article in batch\n",
    "        ]\n",
    "        \n",
    "        batch_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # Process results\n",
    "        for article, (article_results, article_metrics) in zip(batch, batch_results):\n",
    "            if isinstance(article_results, Exception):\n",
    "                print(f\"Error processing article {article['id']}: {article_results}\")\n",
    "                continue\n",
    "                \n",
    "            results.append({\n",
    "                \"article_id\": article[\"id\"],\n",
    "                \"companies\": article_results,\n",
    "                \"metrics\": article_metrics\n",
    "            })\n",
    "            \n",
    "            print(f\"Processed article {article['id']}: {len(article_results)} companies found\")\n",
    "            print(f\"Processing time: {article_metrics['execution_time']:.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the processing\n",
    "results = await process_articles_batch(articles)\n",
    "\n",
    "# Save results\n",
    "with open('extraction_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} articles successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Let's analyze the results and create some visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract metrics for visualization\n",
    "processing_times = [r['metrics']['execution_time'] for r in results]\n",
    "companies_found = [len(r['companies']) for r in results]\n",
    "\n",
    "# Create visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Processing time distribution\n",
    "sns.histplot(processing_times, ax=ax1)\n",
    "ax1.set_title('Processing Time Distribution')\n",
    "ax1.set_xlabel('Time (seconds)')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# Companies found distribution\n",
    "sns.histplot(companies_found, ax=ax2)\n",
    "ax2.set_title('Companies Found Distribution')\n",
    "ax2.set_xlabel('Number of Companies')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
